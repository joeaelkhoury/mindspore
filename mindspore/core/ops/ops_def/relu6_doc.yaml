relu6:
    description: |
        Computes ReLU (Rectified Linear Unit) upper bounded by 6 of input tensors element-wise.

        Refer to :func:`mindspore.ops.relu6` for more details.

        Inputs:
            - **input_x** (Tensor) - Tensor of shape :math:`(N, *)`,
                where :math:`*` means any number of additional dimensions.
                Data type must be float16, float32.

        Outputs:
            Tensor, with the same type and shape as the `input_x`.

        Supported Platforms:
            ``Ascend`` ``GPU`` ``CPU``

        Examples:
            >>> import mindspore
            >>> import numpy as np
            >>> from mindspore import Tensor, ops
            >>> input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)
            >>> relu6 = ops.ReLU6()
            >>> result = relu6(input_x)
            >>> print(result)
            [[0. 4. 0.]
            [2. 0. 6.]]
