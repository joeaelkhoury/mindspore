relu:
    description: |
        Computes ReLU (Rectified Linear Unit activation function) of input tensors element-wise.

        Refer to :func:`mindspore.ops.relu` for more details.
    
        Inputs:
            - **input** (Tensor) - The input tensor.
    
        Outputs:
            Tensor, has the same dtype and shape as `input`.
    
        Supported Platforms:
            ``Ascend`` ``GPU`` ``CPU``
    
        Examples:
            >>> import mindspore
            >>> import numpy as np
            >>> from mindspore import Tensor, ops
            >>> input = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)
            >>> relu = ops.ReLU()
            >>> output = relu(input)
            >>> print(output)
            [[0. 4. 0.]
             [2. 0. 9.]]
