silu:
    description: |
        Computes SiLU (Sigmoid Linear Unit activation function) of input tensors element-wise. Also known as the Swish
        function.

        Refer to :func:`mindspore.ops.silu` for more details.

        Supported Platforms:
            ``Ascend`` ``GPU`` ``CPU``

        Examples:
            >>> x = Tensor(np.array([-1, 2, -3, 2, -1]), mindspore.float16)
            >>> output = ops.silu(x)
            >>> print(output)
            [-0.269  1.762  -0.1423  1.762  -0.269]
